# Knowledge Graph and Text Jointly Embedding论文简要总结
## Align模型在代码层面的理解:
align模型在代码层面的对齐可以理解为2种对齐：
### 1、在knowledge model层面的对齐
facts triplets <h, r, t>中, 头实体h索引ID或者尾实体t索引ID(或者二者)都通过实体名entity name对应的word ID替换, 从而在knowledge model中实现对实体嵌入[entity embedding]和词嵌入[word embedding]的对齐
### 2、在text model[skip-gram]
word pair中<w, v>, 将上下文单词v由其对应的实体ID索引替换, 从而在skip-gram模型中实现实体嵌入[entity embedding]和词嵌入[word embedding]的对齐

### PTransE模型概述
#### 模型中实际有2类模型(knowledge model以及text model), 具体包括4个子模型：
1、knowledge model 1 <h, r, t>, 通过facts三元组建模entity, relation的embedding及其之间的关系
2、konwledge model 2 <Wh, r, Wt>, 对齐单词embedding和实体embedding
3、skip gram 1 <w, v> 建模单词词向量
3、skip gram 2 <w, Ev> 对齐单词embedding和实体embedding

### Skip-Gram模型细节
#### 上下文[context]单词采样, 采样阈值subsample=1e-3[default], 词频大于此阈值, 随机降采样
遵循高频词降采样的原则: p(x) = (sqrt(word_freq/(subsample * total_words)) + 1)*(subsample*total_words/word_freq), 总体是一个单调递减函数, p(x)表示一个被采样的上下文单词的保留概率

#### 训练过程中负采样NEG
基于tf api: tf.nn.fixed_unigram_candidate_sampler()实现基于固定分布的负采样技术

#### 由于负采样存在优化时, 原softmaxloss变成BCELoss
